{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.664185Z",
     "start_time": "2024-08-03T08:00:57.756687Z"
    }
   },
   "source": [
    "import sys \n",
    "import os\n",
    "\n",
    "import torch"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.667475Z",
     "start_time": "2024-08-03T08:00:58.665332Z"
    }
   },
   "cell_type": "code",
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "# Verify the project root path\n",
    "print(\"Project root path:\", project_root)\n",
    "\n",
    "# Add the dora_implementation directory to sys.path\n",
    "sys.path.append(os.path.join(project_root, 'dora_implementation'))\n"
   ],
   "id": "574f584fab585ef7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root path: /Users/vitorsousa/Documents/Projects/dora-implementation\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Try LoRA",
   "id": "beb3e45089357644"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.675420Z",
     "start_time": "2024-08-03T08:00:58.668128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dora_implementation.model.lora_layer import LoRALayer, LinearWithLoRA, LinearWithLoRAMerged\n",
    "from dora_implementation.model.mlp import MultiLayerPerceptron, freeze_linear_layers\n",
    "from dora_implementation.train.train import train\n",
    "from dora_implementation.eval.evaluation import compute_accuracy\n",
    "from torch import nn\n",
    "import torch"
   ],
   "id": "3dc470762e05d3f5",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.707286Z",
     "start_time": "2024-08-03T08:00:58.677183Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## single Linear Layer\n",
    "torch.manual_seed(123)\n",
    "layer = nn.Linear(10, 2)\n",
    "x = torch.randn((1,10))\n",
    "print(\"Original output:\", layer(x))"
   ],
   "id": "91fc6e83815da872",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.713742Z",
     "start_time": "2024-08-03T08:00:58.707858Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## applying LoRA\n",
    "layer_lora_1 = LinearWithLoRA(layer, rank = 2, alpha= 4)\n",
    "print(\"LoRA output:\", layer_lora_1(x))"
   ],
   "id": "84a3320d976f3500",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA output: tensor([[0.6639, 0.4487]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Since we initialized the weight matrix $B$ with zero values in the LoRA layer, the matrix multiplication between $A$ and $B$ result ina matrix consisted of 0's and doesn't affect the original weights.",
   "id": "c4ff104c4b79ba11"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.718854Z",
     "start_time": "2024-08-03T08:00:58.714433Z"
    }
   },
   "cell_type": "code",
   "source": [
    "layer_lora_2 = LinearWithLoRAMerged(layer, rank=2, alpha=4)\n",
    "\n",
    "print(\"LoRA output:\", layer_lora_2(x))"
   ],
   "id": "db08e7145819acb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA output: tensor([[0.6639, 0.4487]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Applying LoRA Layers\n",
    "With the implementation using PyTorch modules enable to easily replace a `Linear` layer in an existing neural network with the `LinearWithLoRA` (or `LinearWithLoRAMerged` layers. \n",
    "\n",
    "We can implement the multiplayer perceptron as follows:\n"
   ],
   "id": "dd6fb57cec759473"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.721624Z",
     "start_time": "2024-08-03T08:00:58.719667Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Hyperparameters\n",
    "random_seed = 123\n",
    "learning_rate = 0.005\n",
    "num_epochs = 2\n",
    "\n",
    "# Architecture\n",
    "num_features = 784\n",
    "num_hidden_1 = 128\n",
    "num_hidden_2 = 256\n",
    "num_classes = 10"
   ],
   "id": "80ec0417d8baed4b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.724475Z",
     "start_time": "2024-08-03T08:00:58.722188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = MultiLayerPerceptron(\n",
    "    num_features = num_features, \n",
    "    num_hidden_1= num_hidden_1, \n",
    "    num_hidden_2= num_hidden_2,\n",
    "    num_classes = num_classes,\n",
    ")"
   ],
   "id": "369220755d8d6eb5",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.727309Z",
     "start_time": "2024-08-03T08:00:58.725325Z"
    }
   },
   "cell_type": "code",
   "source": "print(model)",
   "id": "25ac49c1dac7b97d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=256, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MLP with LoRA layers",
   "id": "a5cc68bb494f881d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.731944Z",
     "start_time": "2024-08-03T08:00:58.729497Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.layers[0] = LinearWithLoRA(model.layers[0], rank=4, alpha=8)\n",
    "model.layers[2] = LinearWithLoRA(model.layers[2], rank=4, alpha=8)\n",
    "model.layers[4] = LinearWithLoRA(model.layers[4], rank=4, alpha=8)"
   ],
   "id": "65086dd4dd0b5adf",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.734420Z",
     "start_time": "2024-08-03T08:00:58.732588Z"
    }
   },
   "cell_type": "code",
   "source": "print(model)",
   "id": "dbd3681e904c145f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiLayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "With this model we can freeze the original `Linear` layers and only make the `LoRALayer` layers trainable as follows:",
   "id": "2c3df71dc169ac93"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:58.737345Z",
     "start_time": "2024-08-03T08:00:58.735242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "freeze_linear_layers(model)\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ],
   "id": "f03e546465a7e7bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Bases on the `True`and `False` values we can confirm that onlye th `LoRA` layers are trainable. ",
   "id": "f9610a4ec8e0a92e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# With an Dataset",
   "id": "226ad39b663b35b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:59.436141Z",
     "start_time": "2024-08-03T08:00:58.738313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time\n",
    "import numpy as np\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch\n"
   ],
   "id": "1a694925932d551",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:59.438428Z",
     "start_time": "2024-08-03T08:00:59.436824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.deterministic = True"
   ],
   "id": "9d0f12189852ecde",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:59.442149Z",
     "start_time": "2024-08-03T08:00:59.439074Z"
    }
   },
   "cell_type": "code",
   "source": "torch.cuda.is_available()",
   "id": "36d35c6ea2e7a4d5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:59.471213Z",
     "start_time": "2024-08-03T08:00:59.442811Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "##########################\n",
    "### MNIST DATASET\n",
    "##########################\n",
    "\n",
    "# Note transforms.ToTensor() scales input images\n",
    "# to 0-1 range\n",
    "train_dataset = datasets.MNIST(root='data', \n",
    "                               train=True, \n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='data', \n",
    "                              train=False, \n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, \n",
    "                          batch_size=BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset=test_dataset, \n",
    "                         batch_size=BATCH_SIZE, \n",
    "                         shuffle=False)\n",
    "\n",
    "# Checking the dataset\n",
    "for images, labels in train_loader:  \n",
    "    print('Image batch dimensions:', images.shape)\n",
    "    print('Image label dimensions:', labels.shape)\n",
    "    break"
   ],
   "id": "f27316c5595a5614",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image batch dimensions: torch.Size([64, 1, 28, 28])\n",
      "Image label dimensions: torch.Size([64])\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:00:59.476759Z",
     "start_time": "2024-08-03T08:00:59.472104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "torch.manual_seed(random_seed)\n",
    "model_pretrained = MultiLayerPerceptron(\n",
    "    num_features=num_features,\n",
    "    num_hidden_1=num_hidden_1,\n",
    "    num_hidden_2=num_hidden_2, \n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "model_pretrained.to(DEVICE)\n",
    "optimizer_pretrained = torch.optim.Adam(model_pretrained.parameters(), lr=learning_rate)"
   ],
   "id": "14801f5a3fb45874",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:01:06.871705Z",
     "start_time": "2024-08-03T08:00:59.477602Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "train(num_epochs, model_pretrained, optimizer_pretrained, train_loader, DEVICE)"
   ],
   "id": "ac2d99df69fb2bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/002 | Batch 000/938 | Loss: 2.2971\n",
      "Epoch: 001/002 | Batch 400/938 | Loss: 0.2084\n",
      "Epoch: 001/002 | Batch 800/938 | Loss: 0.1584\n",
      "Epoch: 001/002 training accuracy: 95.72%\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 002/002 | Batch 000/938 | Loss: 0.0469\n",
      "Epoch: 002/002 | Batch 400/938 | Loss: 0.0589\n",
      "Epoch: 002/002 | Batch 800/938 | Loss: 0.0553\n",
      "Epoch: 002/002 training accuracy: 97.16%\n",
      "Time elapsed: 0.12 min\n",
      "Total Training Time: 0.12 min\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "59f9ac3d7f11c182"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:01:07.139865Z",
     "start_time": "2024-08-03T08:01:06.872635Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'Test accuracy orig model: {compute_accuracy(model_pretrained, test_loader, DEVICE):.2f}%')",
   "id": "728de45492257f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy orig model: 96.82%\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trained with LoRA",
   "id": "e5525679a97b0aa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:01:07.143844Z",
     "start_time": "2024-08-03T08:01:07.140865Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy\n",
    "model_lora = copy.deepcopy(model_pretrained)"
   ],
   "id": "d3b11e01839e961",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:01:07.148738Z",
     "start_time": "2024-08-03T08:01:07.144477Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_lora.layers[0] = LinearWithLoRA(model_lora.layers[0], rank=4, alpha=8)\n",
    "model_lora.layers[2] = LinearWithLoRA(model_lora.layers[2], rank=4, alpha=8)\n",
    "model_lora.layers[4] = LinearWithLoRA(model_lora.layers[4], rank=4, alpha=8)\n",
    "\n",
    "model_lora.to(DEVICE)\n",
    "optimizer_lora = torch.optim.Adam(model_lora.parameters(), lr=learning_rate)\n",
    "model_lora"
   ],
   "id": "ed63b96b3240af22",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (layers): Sequential(\n",
       "    (0): LinearWithLoRA(\n",
       "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): LinearWithLoRA(\n",
       "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "    (3): ReLU()\n",
       "    (4): LinearWithLoRA(\n",
       "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:01:07.151844Z",
     "start_time": "2024-08-03T08:01:07.149606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "freeze_linear_layers(model_lora)\n",
    "\n",
    "# Check if linear layers are frozen\n",
    "for name, param in model_lora.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ],
   "id": "acf1b4bd3c6eee3a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:01:14.698518Z",
     "start_time": "2024-08-03T08:01:07.152603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(num_epochs, model_lora, optimizer_lora, train_loader, DEVICE)\n",
    "print(f'Test accuracy LoRA finetune: {compute_accuracy(model_lora, test_loader, DEVICE):.2f}%')"
   ],
   "id": "49177cbd27ee8e46",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/002 | Batch 000/938 | Loss: 0.1799\n",
      "Epoch: 001/002 | Batch 400/938 | Loss: 0.1726\n",
      "Epoch: 001/002 | Batch 800/938 | Loss: 0.0354\n",
      "Epoch: 001/002 training accuracy: 97.48%\n",
      "Time elapsed: 0.06 min\n",
      "Epoch: 002/002 | Batch 000/938 | Loss: 0.2592\n",
      "Epoch: 002/002 | Batch 400/938 | Loss: 0.1331\n",
      "Epoch: 002/002 | Batch 800/938 | Loss: 0.0404\n",
      "Epoch: 002/002 training accuracy: 97.44%\n",
      "Time elapsed: 0.12 min\n",
      "Total Training Time: 0.12 min\n",
      "Test accuracy LoRA finetune: 96.70%\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Trained with DoRA",
   "id": "f5e9e60451bc9c30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:01:14.702311Z",
     "start_time": "2024-08-03T08:01:14.699273Z"
    }
   },
   "cell_type": "code",
   "source": "from dora_implementation.model.dora_layer import LinearWithDoRA, LinearWithDoRAMerged",
   "id": "966a5f0e97d27536",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:01:14.706130Z",
     "start_time": "2024-08-03T08:01:14.703098Z"
    }
   },
   "cell_type": "code",
   "source": "model_dora = copy.deepcopy(model_pretrained)",
   "id": "10ca2924c33642ec",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:02:39.969203Z",
     "start_time": "2024-08-03T08:02:39.963769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_dora.layers[0] = LinearWithDoRAMerged(model_dora.layers[0], rank=4, alpha=8)\n",
    "model_dora.layers[2] = LinearWithDoRAMerged(model_dora.layers[2], rank=4, alpha=8)\n",
    "model_dora.layers[4] = LinearWithDoRAMerged(model_dora.layers[4], rank=4, alpha=8)\n",
    "\n",
    "model_dora.to(DEVICE)\n",
    "optimizer_dora = torch.optim.Adam(model_dora.parameters(), lr=learning_rate)\n",
    "model_dora"
   ],
   "id": "acaec3884b9244ec",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLayerPerceptron(\n",
       "  (layers): Sequential(\n",
       "    (0): LinearWithDoRAMerged(\n",
       "      (linear): Linear(in_features=784, out_features=128, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): LinearWithDoRAMerged(\n",
       "      (linear): Linear(in_features=128, out_features=256, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "    (3): ReLU()\n",
       "    (4): LinearWithDoRAMerged(\n",
       "      (linear): Linear(in_features=256, out_features=10, bias=True)\n",
       "      (lora): LoRALayer()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:03:28.695816Z",
     "start_time": "2024-08-03T08:03:28.258237Z"
    }
   },
   "cell_type": "code",
   "source": "print(f'Test accuracy DoRA model: {compute_accuracy(model_dora, test_loader, DEVICE):.2f}%')",
   "id": "bd2d55a01616c21c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy DoRA model: 96.82%\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:04:01.408889Z",
     "start_time": "2024-08-03T08:04:01.406517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "freeze_linear_layers(model_dora)\n",
    "\n",
    "for name, param in model_dora.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ],
   "id": "39175f8f4ed9986d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.m: True\n",
      "layers.0.linear.weight: False\n",
      "layers.0.linear.bias: False\n",
      "layers.0.lora.A: True\n",
      "layers.0.lora.B: True\n",
      "layers.2.m: True\n",
      "layers.2.linear.weight: False\n",
      "layers.2.linear.bias: False\n",
      "layers.2.lora.A: True\n",
      "layers.2.lora.B: True\n",
      "layers.4.m: True\n",
      "layers.4.linear.weight: False\n",
      "layers.4.linear.bias: False\n",
      "layers.4.lora.A: True\n",
      "layers.4.lora.B: True\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-03T08:07:01.437836Z",
     "start_time": "2024-08-03T08:06:51.060612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train(num_epochs, model_dora, optimizer_dora, train_loader, DEVICE)\n",
    "print(f'Test accuracy DoRA finetune: {compute_accuracy(model_dora, test_loader, DEVICE):.2f}%')"
   ],
   "id": "fbf24b9925ee16ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001/002 | Batch 000/938 | Loss: 0.1191\n",
      "Epoch: 001/002 | Batch 400/938 | Loss: 0.1147\n",
      "Epoch: 001/002 | Batch 800/938 | Loss: 0.1228\n",
      "Epoch: 001/002 training accuracy: 97.77%\n",
      "Time elapsed: 0.08 min\n",
      "Epoch: 002/002 | Batch 000/938 | Loss: 0.0264\n",
      "Epoch: 002/002 | Batch 400/938 | Loss: 0.0729\n",
      "Epoch: 002/002 | Batch 800/938 | Loss: 0.1104\n",
      "Epoch: 002/002 training accuracy: 98.07%\n",
      "Time elapsed: 0.17 min\n",
      "Total Training Time: 0.17 min\n",
      "Test accuracy DoRA finetune: 97.02%\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a87e72a04cf12269"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
